{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import module\n",
    "import optuna\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "True\n",
      "1\n",
      "12.1\n",
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.version.cuda)\n",
    "print('학습을 진행하는 기기:', torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(\"processed/datasets.pt\")['ABC']['train']\n",
    "valid_dataset = torch.load(\"processed/datasets.pt\")['ABC']['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = []\n",
    "dataset_names = [\"train\", \"valid\"]\n",
    "\n",
    "for name in dataset_names:\n",
    "    dataloader = DataLoader(eval(name + \"_dataset\"), batch_size=1, shuffle=True, drop_last=False)\n",
    "    dataloaders.append(dataloader)\n",
    "\n",
    "train_dataloader, valid_dataloader = dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overweight for positive weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.load(\"processed/splits_stage.pt\")['train']\n",
    "valid = torch.load(\"processed/splits_stage.pt\")['valid']\n",
    "train_id = torch.load(\"processed/splits_stay.pt\")['train']\n",
    "valid_id = torch.load(\"processed/splits_stay.pt\")['valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights_main_train = module.compute_pos_weights_presence(train)\n",
    "rrt_weight_train = module.compute_rrt_pos_weight(train_id)\n",
    "pos_weights_sub_train = module.compute_pos_weights_stage(\n",
    "    train,\n",
    "    stage_cols=[f\"GT_stage_{s}\" for s in [3, 2, 1]],\n",
    "    rrt_weight=rrt_weight_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights_main_valid = module.compute_pos_weights_presence(valid)\n",
    "rrt_weight_valid = module.compute_rrt_pos_weight(valid_id)\n",
    "pos_weights_sub_valid = module.compute_pos_weights_stage(\n",
    "    valid,\n",
    "    stage_cols=[f\"GT_stage_{s}\" for s in [3, 2, 1]],\n",
    "    rrt_weight=rrt_weight_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = [pos_weights_main_train, pos_weights_sub_train, pos_weights_main_valid, pos_weights_sub_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-18 16:20:30,144] A new study created in memory with name: no-name-fb857f00-7bc5-4128-b893-694ebec3c521\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    study  = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(\n",
    "    lambda trial: module.objective(trial, train_dataloader, valid_dataloader, pos_weights, device),\n",
    "    n_trials=50\n",
    "    )\n",
    "\n",
    "    print(\"Best Hyperparameters:\", study.best_params)\n",
    "    print(\"Best Validation Loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_path = \"model/best_params.json\"\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "with open(best_params_path, \"w\") as fp:\n",
    "    json.dump(study.best_params, fp, indent=2)\n",
    "\n",
    "best_trial_num   = study.best_trial.number + 1 \n",
    "best_ckpt_path   = f\"model/trial_{best_trial_num}_model.pt\"\n",
    "\n",
    "with open(\"model/best_ckpt_path.txt\", \"w\") as fp:\n",
    "    fp.write(best_ckpt_path)\n",
    "\n",
    "print(\"✅ Optuna Completed\")\n",
    "print(\"  • Best params saved  ➜\", best_params_path)\n",
    "print(\"  • Best model saved   ➜\", best_ckpt_path)\n",
    "print(\"  • Best valid loss    ➜\", study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tobca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
